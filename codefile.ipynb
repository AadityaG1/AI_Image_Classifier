{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a86e79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: data_fake_real\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = \"data_fake_real\"\n",
    "print(\"Loading dataset from: \" + dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d1d8e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 files belonging to 2 classes.\n",
      "Found 20000 files belonging to 2 classes.\n",
      "Training Classes:\n",
      "['FAKE', 'REAL']\n",
      "Testing Classes:\n",
      "['FAKE', 'REAL']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "img_height = 32 \n",
    "img_width = 32\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Loading training data\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  dataset_dir + \"/train\",\n",
    "  seed=10,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=BATCH_SIZE)\n",
    "\n",
    "# Loading validation data\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  dataset_dir + \"/test\",\n",
    "  seed=10,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=BATCH_SIZE)\n",
    "\n",
    "# Checking if it is loaded properly\n",
    "print(\"Training Classes:\")\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)\n",
    "\n",
    "print(\"Testing Classes:\")\n",
    "class_names = val_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26d7a987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 32, 32, 32)        128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 32, 32, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 16, 16, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 16, 16, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 16, 16, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 16, 16, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 8, 8, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 8192)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               2097408   \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2340929 (8.93 MB)\n",
      "Trainable params: 2339713 (8.93 MB)\n",
      "Non-trainable params: 1216 (4.75 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Building the architecture \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "model = Sequential()\n",
    "    \n",
    "# Block 1\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Block 2\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "# Final Classifier Block\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "# Summary\n",
    "model.build(input_shape=(None, 32, 32, 3))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e880b541",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train) = next(iter(train_ds))\n",
    "(x_val, y_val) = next(iter(val_ds))\n",
    "tf.experimental.numpy.experimental_enable_numpy_behavior()\n",
    "x_train_reshaped = x_train.reshape(-1, 32, 32, 3)\n",
    "x_val_reshaped = x_val.reshape(-1, 32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d83013c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 32, 3)\n",
      "(32,)\n",
      "(32, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_train_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1922f3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/5\n",
      "3125/3125 [==============================] - 2023s 647ms/step - loss: 0.2422 - accuracy: 0.9020 - val_loss: 0.2182 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "3125/3125 [==============================] - 1904s 609ms/step - loss: 0.1809 - accuracy: 0.9295 - val_loss: 0.1748 - val_accuracy: 0.9323 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "3125/3125 [==============================] - 1972s 631ms/step - loss: 0.1614 - accuracy: 0.9374 - val_loss: 0.1832 - val_accuracy: 0.9281 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "3125/3125 [==============================] - 1950s 624ms/step - loss: 0.1422 - accuracy: 0.9461 - val_loss: 0.3513 - val_accuracy: 0.8722 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "3125/3125 [==============================] - 2020s 646ms/step - loss: 0.1293 - accuracy: 0.9507 - val_loss: 0.1822 - val_accuracy: 0.9298 - lr: 0.0010\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#Defining callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "\n",
    "callbacks_list = [early_stopping, reduce_lr]\n",
    "\n",
    "# Data augmentation\n",
    "data_generator = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    shear_range=0.2,\n",
    "    fill_mode='nearest'\n",
    "\n",
    ")\n",
    "\n",
    "train_generator = data_generator.flow(x_train_reshaped, y_train, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "print(\"Starting training...\")\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=(x_val_reshaped, y_val),\n",
    "  epochs=5,\n",
    "  callbacks=callbacks_list,\n",
    "  verbose=1\n",
    ")\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04db6cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 122s 195ms/step - loss: 0.1822 - accuracy: 0.9298\n",
      "Test loss:  0.1821587234735489\n",
      "Test accuracy: 0.9298499822616577\n"
     ]
    }
   ],
   "source": [
    "#Evaluating model performance\n",
    "l, a = model.evaluate(val_ds, verbose=1)\n",
    "print(\"Test loss: \", l)\n",
    "print(\"Test accuracy:\", a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5c218d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('9-6-train-0.95-5epoxhs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af0e27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('13-06-test-0.93-5epochs.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
